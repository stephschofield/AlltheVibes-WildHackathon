# Ollama server URL (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Model to use (must be pulled first with: ollama pull <model>)
# Good options with tool-calling support:
#   qwen2.5:7b     - Fast, good tool use (recommended)
#   qwen2.5:1.5b   - Fastest, for CPU-only setups
#   llama3.1:8b    - Meta's model, solid tool support
#   mistral:7b     - Good quality, fast
OLLAMA_MODEL=qwen2.5:7b
